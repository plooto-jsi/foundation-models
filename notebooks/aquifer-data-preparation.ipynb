{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquifer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopy pandas sqlite3 folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Geographical data plotting\n",
    "import folium\n",
    "from folium.map import Popup\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from geopy.distance import distance\n",
    "\n",
    "# Time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('../data/external/data.db')\n",
    "\n",
    "# Get a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# List all of the tables\n",
    "tables = cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "print(\"Tables in the database:\", table_names)\n",
    "\n",
    "# Load data from each table into a pandas DataFrame\n",
    "dataframes = {table: pd.read_sql_query(f\"SELECT * FROM {table}\", conn) for table in table_names}\n",
    "\n",
    "# Name the datasets that we are going to work with\n",
    "aquifer_stations = dataframes[table_names[4]]\n",
    "aquifer_measurements = dataframes[table_names[5]]\n",
    "weather_locations = dataframes[table_names[6]]\n",
    "weather = dataframes[table_names[7]]\n",
    "\n",
    "print(\"aquifer stations:\")\n",
    "print(aquifer_stations.head())\n",
    "print(\"aquifer measurements:\")\n",
    "print(aquifer_measurements.head())\n",
    "print(\"weather stations:\")\n",
    "print(weather_locations.head())\n",
    "print(\"Weather measurements:\")\n",
    "print(weather.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquifer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime\n",
    "aquifer_measurements['date'] = pd.to_datetime(aquifer_measurements['date'])\n",
    "\n",
    "# Filter data for years 2010 to 2017\n",
    "aquifer_measurements = aquifer_measurements[aquifer_measurements['date'].dt.year.between(2010, 2017)]\n",
    "\n",
    "# Group by station_id and count the number of unique years\n",
    "station_years_counts = aquifer_measurements.groupby('station_id')['date'].apply(lambda x: x.dt.year.nunique())\n",
    "\n",
    "# Filter station_ids that have instances in all years from 2010 to 2017\n",
    "station_ids_all_years = station_years_counts[station_years_counts == 8].index.tolist()\n",
    "\n",
    "# Filter aquifer_measurements to include only stations with instances in all years from 2010 to 2017\n",
    "aquifer_measurements = aquifer_measurements[aquifer_measurements['station_id'].isin(station_ids_all_years)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(aquifer_measurements)\n",
    "\n",
    "\n",
    "# Filter the locations file, so it only includes the remaining stations\n",
    "aquifer_stations = aquifer_stations[aquifer_stations['id'].isin(station_ids_all_years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all of the aquifer stations that are in final selction\n",
    "aquifer_stations.to_csv(\"aquifer_stations.csv\", index=False)\n",
    "\n",
    "# Print filtered station ids\n",
    "print(len(station_ids_all_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column from Unix timestamp to datetime\n",
    "weather['time'] = pd.to_datetime(weather['time'], unit='s')\n",
    "\n",
    "# Group by location_id and count the number of unique years\n",
    "location_years_counts = weather.groupby('location_id')['time'].apply(lambda x: x.dt.year.nunique())\n",
    "\n",
    "# Filter location_ids that have instances in all years from 2010 to 2017\n",
    "location_ids_all_years = location_years_counts[location_years_counts == 8].index.tolist()\n",
    "\n",
    "# Filter weather to include only location with instances in all years from 2010 to 2017\n",
    "weather = weather[weather['location_id'].isin(location_ids_all_years)]\n",
    "\n",
    "# Save as a .csv file\n",
    "weather.to_csv(\"weather.csv\", index=False)\n",
    "print(weather)\n",
    "\n",
    "# Fetch all of the location_id's\n",
    "location_ids = weather['location_id'].unique()\n",
    "\n",
    "print(\"Locations with instances all years from 2010 to 2017:\")\n",
    "print(len(location_ids))\n",
    "\n",
    "# Filter the locations that are in the final locations\n",
    "weather_locations = weather_locations[weather_locations['id'].isin(location_ids_all_years)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values are there in the dataframes\n",
    "print(weather.isna().sum().sum())\n",
    "print(aquifer_measurements.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns with most missing values\n",
    "weather = weather.drop('sun_duration', axis=1)\n",
    "weather = weather.drop('snow_depth', axis=1)\n",
    "\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_csv(\"weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows are there for each id\n",
    "row_count = weather.groupby('location_id').size()\n",
    "print(row_count)\n",
    "\n",
    "# Compare row counts with 2991\n",
    "different_counts = row_count[row_count != 2922]\n",
    "print(\"Different counts:\")\n",
    "print(different_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column level\n",
    "aquifer_measurements = aquifer_measurements.drop('level', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows are there for each id\n",
    "row_count_aq = aquifer_measurements.groupby('station_id').size()\n",
    "print(row_count_aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare row counts with 2850\n",
    "over_treshold = row_count_aq[row_count_aq >= 2850]\n",
    "\n",
    "# Keep only these id's\n",
    "ids_to_keep = over_treshold.index\n",
    "\n",
    "#Filter the aquifer_measurements\n",
    "aquifer_measurements = aquifer_measurements[aquifer_measurements['station_id'].isin(ids_to_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'time' column to 'date'\n",
    "weather = weather.rename(columns={'time': 'date'})\n",
    "\n",
    "# Remove the hour from date\n",
    "weather['date'] = weather['date'].dt.date\n",
    "\n",
    "# Cast aquifer_measurements['date'] to date_time\n",
    "aquifer_measurements['date'] = pd.to_datetime(aquifer_measurements['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataframes based on the stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with dataframes for specific aquifer stations\n",
    "aquifer_measurements_by_stations = {aquifer: data for aquifer, data in aquifer_measurements.groupby('station_id')}\n",
    "\n",
    "# Make a dictionary with dataframes for specific weather locations\n",
    "weather_by_locations = {location: data for location, data in weather.groupby('location_id')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the aquifer_stations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the parentheses and their contents from the 'location' column\n",
    "aquifer_stations['name'] = aquifer_stations['name'].str.replace(r'\\s*\\(.*?\\)', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the stations, so it only includes the remaining stations\n",
    "aquifer_stations = aquifer_stations[aquifer_stations['id'].isin(ids_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the pattern in the 'name' column\n",
    "aquifer_stations['name'] = aquifer_stations['name'].str.replace(r'^Lj[^-]*-', 'Ljubljana ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize geocoder with a specific user-agent\n",
    "geolocator = Nominatim(user_agent=\"geocoder_for_slovenia\")\n",
    "\n",
    "# Function to geocode a place name with retries\n",
    "def geocode_place(place, retries=3, delay=1):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            location = geolocator.geocode(place)\n",
    "            time.sleep(delay) #nominatim supports only 1 query/second\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            else:\n",
    "                return None, None\n",
    "        except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "            print(f\"Error geocoding {place}: {e}, retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    return None, None\n",
    "# Geocode each station name\n",
    "aquifer_stations['latitude'], aquifer_stations['longitude'] = zip(*aquifer_stations['name'].apply(geocode_place))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifer_stations.to_csv(\"aquifer_stations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the aquifer stations and weather locations on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a map centered on Slovenia\n",
    "slovenia_map = folium.Map(location=[46.151241, 14.995463], zoom_start=8)\n",
    "\n",
    "# Add weather locations to the map (red colour)\n",
    "for _, row in weather_locations.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        popup=row['id'],\n",
    "        icon=folium.Icon(color='red')\n",
    "    ).add_to(slovenia_map)\n",
    "\n",
    "# Add aquifer stations to the map (blue colour)\n",
    "for _, row in aquifer_stations.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['id'],\n",
    "        icon=folium.Icon(color='blue')\n",
    "    ).add_to(slovenia_map)\n",
    "\n",
    "\n",
    "# Save the map to an HTML file\n",
    "slovenia_map.save('../data/interim/slovenia_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the weather and aquifer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest weather stations to the aquifer stations\n",
    "\n",
    "# Initialize dictionary to store closest locations\n",
    "closest_locations = {}\n",
    "\n",
    "# Iterate through each location in aquifer_stations\n",
    "for idx1, row1 in aquifer_stations.iterrows():\n",
    "    closest_location = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    # Iterate through each location in weather_locations\n",
    "    for idx2, row2 in weather_locations.iterrows():\n",
    "        # Calculate distance using geopy\n",
    "        dist = distance((row1['latitude'], row1['longitude']), (row2['lat'], row2['lng'])).km\n",
    "        \n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            closest_location = row2['id']\n",
    "    \n",
    "    # Store closest location_id in dictionary\n",
    "    closest_locations[row1['id']] = closest_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(closest_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dictionaries\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to store combined dataframes\n",
    "combined_data = {}\n",
    "\n",
    "# Iterate through mapping_dict and merge corresponding dataframes based on 'date'\n",
    "for id1, loc_key in closest_locations.items():\n",
    "    if id1 in aquifer_measurements_by_stations and loc_key in weather_by_locations:\n",
    "        df1 = aquifer_measurements_by_stations[id1]\n",
    "        df2 = weather_by_locations[loc_key]\n",
    "        \n",
    "        # Cast both 'date' columns to datetime\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "        df2['date'] = pd.to_datetime(df2['date'])\n",
    "\n",
    "        # Merge dataframes based on 'date'\n",
    "        merged_df = pd.merge(df1, df2, on='date', how='outer')\n",
    "        \n",
    "        # Store merged dataframe in combined_data\n",
    "        combined_data[id1] = merged_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values based on stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in combined_data.items():\n",
    "    print(f\"{key}: {data.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = []\n",
    "for key, data in combined_data.items():\n",
    "    num_missing = data.isna().sum().sum()\n",
    "    if num_missing < 200:\n",
    "        missing_values.append(key)\n",
    "print(len(combined_data.keys()))\n",
    "print(len(missing_values))\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data[30015].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dictionary to only include keys in the list\n",
    "combined_data = {key: combined_data[key] if key in combined_data else None for key in missing_values}\n",
    "\n",
    "for key, data in combined_data.items():\n",
    "    print(f\"{key}: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift one feature for 3 days and fill the NaNs with the first value in the column\n",
    "\n",
    "'''first_value = combined_data[10005]['temperature_avg'].iloc[0]\n",
    "# Create a new column with values shifted by 3 days ahead\n",
    "combined_data[10005]['temperature_avg_shift3'] = combined_data[10005]['temperature_avg'].shift(3)\n",
    "combined_data[10005]['temperature_avg_shift3'] = combined_data[10005]['temperature_avg_shift3'].fillna(first_value)\n",
    "print(combined_data[10005]['temperature_avg_shift3'])\n",
    "print(combined_data[10005]['temperature_avg'].iloc[0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new column altitude_diff that contains the altitude difference\n",
    "# between consecutive days\n",
    "\n",
    "for key, data in combined_data.items():\n",
    "    data['altitude_diff'] = data['altitude'].diff()\n",
    "    data['altitude_diff'] = data['altitude_diff'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting the appropriate columns for 1 to 10 days ahead\n",
    "\n",
    "# Specify the columns to shift\n",
    "columns_to_shift = ['day_time', 'precipitation', 'snow_accumulation', 'temperature_avg',\n",
    "       'temperature_min', 'temperature_max', 'cloud_cover_avg',\n",
    "       'cloud_cover_min', 'cloud_cover_max', 'dew_point_avg', 'dew_point_min',\n",
    "       'dew_point_max', 'humidity_avg', 'humidity_min', 'humidity_max',\n",
    "       'pressure_avg', 'pressure_min', 'pressure_max', 'uv_index_avg',\n",
    "       'uv_index_min', 'uv_index_max', 'precipitation_probability_avg',\n",
    "       'precipitation_probability_min', 'precipitation_probability_max',\n",
    "       'precipitation_intensity_avg', 'precipitation_intensity_min',\n",
    "       'precipitation_intensity_max', 'altitude_diff']\n",
    "\n",
    "# Iterate over all of the dataframes in the dictionary\n",
    "for key, data in combined_data.items():\n",
    "    # Iterate over all of the columns in the columns_to_shift\n",
    "    for column in columns_to_shift:\n",
    "        # Iterate over all shifts\n",
    "        for shift in range (1, 16):\n",
    "            first_value = data[column].iloc[0]\n",
    "            data[f'{column}_shift{shift}'] = data[column].shift(shift)\n",
    "            # Fill the first values (NaN) with the first values from original columns\n",
    "            data[f'{column}_shift{shift}'] = data[f'{column}_shift{shift}'].fillna(first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the averages for all of the features (2 to 10 days)\n",
    "\n",
    "# Specify the columns to average\n",
    "columns_to_average = ['day_time', 'precipitation', 'snow_accumulation', 'temperature_avg',\n",
    "       'temperature_min', 'temperature_max', 'cloud_cover_avg',\n",
    "       'cloud_cover_min', 'cloud_cover_max', 'dew_point_avg', 'dew_point_min',\n",
    "       'dew_point_max', 'humidity_avg', 'humidity_min', 'humidity_max',\n",
    "       'pressure_avg', 'pressure_min', 'pressure_max', 'uv_index_avg',\n",
    "       'uv_index_min', 'uv_index_max', 'precipitation_probability_avg',\n",
    "       'precipitation_probability_min', 'precipitation_probability_max',\n",
    "       'precipitation_intensity_avg', 'precipitation_intensity_min',\n",
    "       'precipitation_intensity_max', 'altitude_diff']\n",
    "\n",
    "# Iterate over all of the dataframes in the dictionary\n",
    "for key, data in combined_data.items():\n",
    "    # Iterate over all of the columns in the columns_to_average\n",
    "    for column in columns_to_average:\n",
    "        # Iterate over all average window sizes\n",
    "        for average in range (2, 16):\n",
    "            data[f'{column}_average{average}'] = data[column].rolling(window=average, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('columns.txt', 'w') as file:\n",
    "    # Iterate through the list elements\n",
    "    for item in combined_data[10005].columns:\n",
    "        # Write each element to the file followed by a newline\n",
    "        file.write(item + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
